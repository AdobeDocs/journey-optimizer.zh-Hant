---
product: experience platform
solution: Experience Platform
title: 個人化最佳化模型
description: 進一步瞭解個人化最佳化模型
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: 1c7bcffe-5a25-444f-8a95-057b7a07f252
version: Journey Orchestration
source-git-commit: 0b94bfeaf694e8eaf0dd85e3c67ee97bd9b56294
workflow-type: tm+mt
source-wordcount: '943'
ht-degree: 5%

---

# 個人化最佳化模型 {#personalized-optimization-model}

## 概觀 {#overview}

個人化最佳化會利用監督機器學習和深度學習領域的先進技術，讓商業使用者 (行銷人員) 定義業務目標，並利用他們的客戶資料來訓練商業導向的模型，以提供個人化產品建議並將 KPI 最大化。

<!--![](../../rn/assets/do-not-localize/ai-ranking.gif)-->

## 資料集需求

若要訓練個人化最佳化模型，資料集必須符合下列最低需求：

* 資料集中至少有2個選件在過去30天內必須具有至少250個顯示事件和25個成功事件（例如，點按或轉換）。
* 過去30天內顯示少於250個和/或25個成功事件的優惠方案符合納入個人化流量的資格，但個人化模型會將其視為在最差評分*優惠方案層級執行，直到超過此臨界值為止。
* 過去30天內顯示少於250個及/或25個成功事件的優惠方案，仍符合納入探索流量的資格。

在第一次訓練個人化最佳化模型之前，會隨機提供使用個人化最佳化模型之選擇策略中的選件。

## 主要模型假設和限制 {#key}

為了最大化使用個人化最佳化的優勢，請注意一些重要假設和限制。

* **優惠方案差異足夠大，使用者在所考慮的優惠方案之間會有不同的偏好設定**。 如果選件太類似，則產生的模型影響會較小，因為回應似乎是隨機的。
例如，如果銀行有兩個信用卡優惠方案，唯一差異是顏色，那麼建議使用哪張卡可能無關緊要，但如果每張卡片的條款不同，這就能解釋為什麼某些客戶會選擇一張卡片，並提供足夠的優惠方案差異，以建立更有影響力的模型。
* **使用者流量構成穩定**。 如果使用者流量構成在模型訓練和預測期間發生大幅變更，模型效能可能會降低。 例如，假設在模型訓練階段中，只有對象A中使用者的資料可用，但已訓練的模型用於產生對象B中使用者的預測，則模型效能可能會受到影響。
* **此模型每週更新，且模型更新時傳遞效能的變更，因此短時間內的優惠效能不會大幅變更**。 例如，某產品之前非常受歡迎，但公開報告指出該產品對我們的健康有害，並且此產品很快變得不受歡迎。 在此案例中，模型可以繼續預測此產品，直到模型更新為使用者行為的變化。

## 運作方式 {#how}

該模型會學習優惠方案、使用者資訊和內容資訊之間的複雜功能互動，以向一般使用者建議個人化優惠方案。 特徵是模型的輸入。

共有3種功能：

| 功能型別 | 如何新增特徵至模型 |
|--------------|----------------------------|
| 決策物件(placementID、activityID、decisionScopeID) | 傳送至AEP的決定管理回饋體驗事件的一部分 |
| 客群 | 建立排名AI模型時，可以新增0到50個對象作為功能 |
| 上下文資料 | 傳送至AEP的決定回饋體驗事件的一部分。 可新增至結構描述的可用內容資料： Commerce詳細資料、通道詳細資料、應用程式詳細資料、Web詳細資料、環境詳細資料、裝置詳細資料、placeContext |

模型有兩個階段：

* 在&#x200B;**離線模型訓練**&#x200B;階段中，藉由學習並記憶歷史資料中的功能互動來訓練模型。
* 在&#x200B;**線上推斷**&#x200B;階段中，候選者優惠方案會根據模型產生的即時分數進行排名。 傳統協同篩選技術很難加入功能供使用者和選件使用，而個人化最佳化屬於深度學習型建議方法，可加入並學習複雜及非線性功能的互動模式。

以下是一個簡化的範例，說明個人化最佳化背後的基本概念。 假設我們有資料集，用來儲存使用者和選件之間的歷史互動，如圖1所示。 有：

* 兩個選件，offer_1和offer_2，
* 兩個特徵，feature_1和feature_2，
* 回應欄。

feature_1、feature_2和回應的值是0或1。 檢視圖1中的藍色方框和橘色方框時，我們可以發現，對於offer_1，當feature_1和feature_2具有相同的值時，回應更有可能是1，而對於offer_2，當feature_1為0和feature_2為1時，標籤更有可能是1。 我們也可以看到在紅色方塊中，當feature_1為0且feature_2為1且回應為0時，會提供offer_1。 根據我們在橘色方塊中看到的模式，當feature_1為0且feature_2為1時，offer_2可能是較好的建議。

基本上，這是學習和記憶歷史功能互動，並套用這些互動以產生個人化預測的構想。

![](../assets/perso-ranking-schema.png)

## 冷啟動問題 {#cold-start}

當沒有足夠的資料來進行建議時，就會發生冷啟動問題。 針對個人化最佳化，冷啟動問題有兩種型別。

* **建立不含歷史資料的新AI模型後**，系統會隨機提供一段時間選件以收集資料，且資料會用於訓練第一個模型。
* **第一個模型發行後**，總流量的10%將配置給隨機服務，而90%的流量將用於模型建議。 因此，如果新選件新增至AI模型，則會作為10%流量的一部分提供。 隨著模型持續更新，針對這些優惠方案收集的資料將決定從90%的流量中選取的次數。

## 重新訓練 {#re-training}

模型將接受重新培訓，以學習最新功能互動，並每週減緩模型效能的下降。
